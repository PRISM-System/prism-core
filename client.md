# PRISM-Core í´ë¼ì´ì–¸íŠ¸ ê°€ì´ë“œ

PRISM-Coreë¥¼ ì´ìš©í•´ AI ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ê³  í™œìš©í•˜ëŠ” ë‹¨ê³„ë³„ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [ê¸°ë³¸ ì„¤ì •](#ê¸°ë³¸-ì„¤ì •)
2. [Vector DB í™œìš©](#vector-db-í™œìš©)
3. [LLM ì„œë¹„ìŠ¤ ì—°ë™](#llm-ì„œë¹„ìŠ¤-ì—°ë™)
4. [Tool ì‹œìŠ¤í…œ í™œìš©](#tool-ì‹œìŠ¤í…œ-í™œìš©)
5. [ì‹¤ì œ ì˜ˆì œ](#ì‹¤ì œ-ì˜ˆì œ)

## ğŸ”§ ê¸°ë³¸ ì„¤ì •

### 1. PRISM-Core ì„œë²„ ì‹œì‘

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/PRISM-System/prism-core.git
cd prism-core

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ì—ì„œ í•„ìš”í•œ ì„¤ì • ìˆ˜ì •

# ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d
```

### 2. í´ë¼ì´ì–¸íŠ¸ í™˜ê²½ ì„¤ì •

```bash
# Python ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ë˜ëŠ” venv\Scripts\activate  # Windows

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install requests fastapi uvicorn
```

### 3. ì—°ê²° í…ŒìŠ¤íŠ¸

```python
import requests

# ì„œë²„ ìƒíƒœ í™•ì¸
response = requests.get("http://localhost:8000/")
print(response.json())
# ì˜ˆìƒ ì¶œë ¥: {"message": "Welcome to PRISM Core", "version": "0.1.0"}
```

## ğŸ§  Vector DB í™œìš©

### 1. ì¸ë±ìŠ¤ ìƒì„±

```python
import requests

def create_index(class_name: str, description: str = ""):
    """Vector DB ì¸ë±ìŠ¤ ìƒì„±"""
    url = "http://localhost:8000/api/vector-db/indices"
    data = {
        "class_name": class_name,
        "description": description,
        "vector_dimension": 384,
        "encoder_model": "sentence-transformers/all-MiniLM-L6-v2"
    }
    
    response = requests.post(url, json=data)
    return response.json()

# ì‚¬ìš© ì˜ˆì‹œ
create_index("Documents", "ë¬¸ì„œ ì €ì¥ì†Œ")
create_index("Knowledge", "ì§€ì‹ ë² ì´ìŠ¤")
```

### 2. ë¬¸ì„œ ì¶”ê°€

```python
def add_documents(class_name: str, documents: list):
    """ë¬¸ì„œë¥¼ Vector DBì— ì¶”ê°€"""
    url = f"http://localhost:8000/api/vector-db/documents/{class_name}/batch"
    
    response = requests.post(url, json=documents)
    return response.json()

# ì‚¬ìš© ì˜ˆì‹œ
documents = [
    {
        "title": "ë°˜ë„ì²´ ì œì¡° ê³µì •",
        "content": "ë°˜ë„ì²´ ì œì¡°ëŠ” ì›¨ì´í¼ ì¤€ë¹„ë¶€í„° íŒ¨í‚¤ì§•ê¹Œì§€ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì¹©ë‹ˆë‹¤.",
        "metadata": {"category": "manufacturing", "source": "manual"}
    },
    {
        "title": "í’ˆì§ˆ ê´€ë¦¬ ê°€ì´ë“œ",
        "content": "í’ˆì§ˆ ê´€ë¦¬ëŠ” ì œí’ˆì˜ ì¼ê´€ì„±ê³¼ ì‹ ë¢°ì„±ì„ ë³´ì¥í•˜ëŠ” ì¤‘ìš”í•œ ê³¼ì •ì…ë‹ˆë‹¤.",
        "metadata": {"category": "quality", "source": "guide"}
    }
]

add_documents("Documents", documents)
```

### 3. ë¬¸ì„œ ê²€ìƒ‰

```python
def search_documents(query: str, class_name: str = "Documents", limit: int = 5):
    """ë¬¸ì„œ ê²€ìƒ‰"""
    url = f"http://localhost:8000/api/vector-db/search/{class_name}"
    data = {
        "query": query,
        "limit": limit
    }
    
    response = requests.post(url, json=data)
    return response.json()

# ì‚¬ìš© ì˜ˆì‹œ
results = search_documents("ë°˜ë„ì²´ ì œì¡° ê³µì •", "Documents", 3)
for result in results:
    print(f"ì ìˆ˜: {result['score']:.3f}")
    print(f"ë‚´ìš©: {result['content']}")
    print("---")
```

## ğŸ¤– LLM ì„œë¹„ìŠ¤ ì—°ë™

### 1. í…ìŠ¤íŠ¸ ìƒì„±

```python
def generate_text(prompt: str, model: str = "Qwen/Qwen3-0.6B"):
    """LLMì„ ì´ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„±"""
    url = "http://localhost:8001/v1/chat/completions"
    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 500,
        "temperature": 0.7
    }
    
    response = requests.post(url, json=data)
    return response.json()

# ì‚¬ìš© ì˜ˆì‹œ
response = generate_text("ë°˜ë„ì²´ ì œì¡° ê³µì •ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
print(response['choices'][0]['message']['content'])
```

### 2. ëŒ€í™”í˜• ì±—ë´‡

```python
class ChatBot:
    def __init__(self, model: str = "Qwen/Qwen3-0.6B"):
        self.model = model
        self.conversation_history = []
        self.base_url = "http://localhost:8001/v1/chat/completions"
    
    def chat(self, message: str):
        """ëŒ€í™”í˜• ì‘ë‹µ ìƒì„±"""
        self.conversation_history.append({"role": "user", "content": message})
        
        data = {
            "model": self.model,
            "messages": self.conversation_history,
            "max_tokens": 300,
            "temperature": 0.7
        }
        
        response = requests.post(self.base_url, json=data)
        assistant_message = response.json()['choices'][0]['message']['content']
        
        self.conversation_history.append({"role": "assistant", "content": assistant_message})
        return assistant_message
    
    def reset(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.conversation_history = []

# ì‚¬ìš© ì˜ˆì‹œ
bot = ChatBot()
response = bot.chat("ì•ˆë…•í•˜ì„¸ìš”!")
print(response)
```

## ğŸ”§ Tool ì‹œìŠ¤í…œ í™œìš©

### 1. Tool ë“±ë¡

```python
def register_tool(tool_config: dict):
    """ìƒˆë¡œìš´ Tool ë“±ë¡"""
    url = "http://localhost:8000/api/tools/register"
    
    response = requests.post(url, json=tool_config)
    return response.json()

# ì‚¬ìš© ì˜ˆì‹œ
calculator_tool = {
    "name": "calculator",
    "description": "ìˆ˜í•™ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë„êµ¬",
    "parameters_schema": {
        "type": "object",
        "properties": {
            "expression": {
                "type": "string",
                "description": "ê³„ì‚°í•  ìˆ˜í•™ í‘œí˜„ì‹"
            }
        },
        "required": ["expression"]
    },
    "tool_type": "calculation",
    "config": {
        "safe_mode": True
    }
}

register_tool(calculator_tool)
```

### 2. Tool ì‹¤í–‰

```python
def execute_tool(tool_name: str, parameters: dict):
    """Tool ì‹¤í–‰"""
    url = "http://localhost:8000/api/tools/execute"
    data = {
        "tool_name": tool_name,
        "parameters": parameters
    }
    
    response = requests.post(url, json=data)
    return response.json()

# ì‚¬ìš© ì˜ˆì‹œ
result = execute_tool("calculator", {"expression": "2 + 3 * 4"})
print(result)
```

### 3. ë“±ë¡ëœ Tool ì¡°íšŒ

```python
def list_tools():
    """ë“±ë¡ëœ Tool ëª©ë¡ ì¡°íšŒ"""
    url = "http://localhost:8000/api/tools"
    
    response = requests.get(url)
    return response.json()

# ì‚¬ìš© ì˜ˆì‹œ
tools = list_tools()
for tool in tools:
    print(f"Tool: {tool['name']}")
    print(f"ì„¤ëª…: {tool['description']}")
    print("---")
```

## ğŸ“ ì‹¤ì œ ì˜ˆì œ

### 1. ë¬¸ì„œ ê¸°ë°˜ QA ì‹œìŠ¤í…œ

```python
class DocumentQA:
    def __init__(self, vector_db_class: str = "Documents"):
        self.vector_db_class = vector_db_class
        self.base_url = "http://localhost:8000"
        self.llm_url = "http://localhost:8001/v1/chat/completions"
    
    def answer_question(self, question: str):
        """ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€"""
        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        search_results = self._search_documents(question)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(search_results)
        
        # 3. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        answer = self._generate_answer(question, context)
        
        return {
            "question": question,
            "answer": answer,
            "sources": search_results
        }
    
    def _search_documents(self, query: str):
        """ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        url = f"{self.base_url}/api/vector-db/search/{self.vector_db_class}"
        data = {"query": query, "limit": 3}
        
        response = requests.post(url, json=data)
        return response.json()
    
    def _build_context(self, search_results: list):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±"""
        context_parts = []
        for result in search_results:
            context_parts.append(f"ì œëª©: {result.get('title', '')}")
            context_parts.append(f"ë‚´ìš©: {result.get('content', '')}")
            context_parts.append("---")
        
        return "\n".join(context_parts)
    
    def _generate_answer(self, question: str, context: str):
        """LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        
        data = {
            "model": "Qwen/Qwen3-0.6B",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 400,
            "temperature": 0.7
        }
        
        response = requests.post(self.llm_url, json=data)
        return response.json()['choices'][0]['message']['content']

# ì‚¬ìš© ì˜ˆì‹œ
qa_system = DocumentQA("Documents")
result = qa_system.answer_question("ë°˜ë„ì²´ ì œì¡° ê³µì •ì˜ ì£¼ìš” ë‹¨ê³„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")
print(f"ì§ˆë¬¸: {result['question']}")
print(f"ë‹µë³€: {result['answer']}")
```

### 2. ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸

```python
class IntelligentAgent:
    def __init__(self, name: str):
        self.name = name
        self.tools = []
        self.knowledge_base = "Knowledge"
        self.base_url = "http://localhost:8000"
        self.llm_url = "http://localhost:8001/v1/chat/completions"
    
    def add_tool(self, tool_name: str):
        """ì—ì´ì „íŠ¸ì— Tool ì¶”ê°€"""
        self.tools.append(tool_name)
    
    def process_request(self, request: str):
        """ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬"""
        # 1. ìš”ì²­ ë¶„ì„
        analysis = self._analyze_request(request)
        
        # 2. í•„ìš”í•œ Tool ê²°ì •
        required_tools = self._determine_tools(analysis)
        
        # 3. ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰
        knowledge = self._search_knowledge(request)
        
        # 4. ì‘ë‹µ ìƒì„±
        response = self._generate_response(request, analysis, required_tools, knowledge)
        
        return response
    
    def _analyze_request(self, request: str):
        """ìš”ì²­ ë¶„ì„"""
        prompt = f"""ë‹¤ìŒ ìš”ì²­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
ìš”ì²­: {request}

ë¶„ì„ ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
- intent: ì˜ë„
- required_tools: í•„ìš”í•œ ë„êµ¬ë“¤
- complexity: ë³µì¡ë„ (simple/medium/complex)
"""
        
        data = {
            "model": "Qwen/Qwen3-0.6B",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 200,
            "temperature": 0.3
        }
        
        response = requests.post(self.llm_url, json=data)
        return response.json()['choices'][0]['message']['content']
    
    def _determine_tools(self, analysis: str):
        """í•„ìš”í•œ Tool ê²°ì •"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ Tool ë§¤ì¹­
        tools_needed = []
        analysis_lower = analysis.lower()
        
        if "ê³„ì‚°" in analysis_lower or "ìˆ˜í•™" in analysis_lower:
            tools_needed.append("calculator")
        
        return tools_needed
    
    def _search_knowledge(self, query: str):
        """ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰"""
        url = f"{self.base_url}/api/vector-db/search/{self.knowledge_base}"
        data = {"query": query, "limit": 2}
        
        response = requests.post(url, json=data)
        return response.json()
    
    def _generate_response(self, request: str, analysis: str, tools: list, knowledge: list):
        """ì‘ë‹µ ìƒì„±"""
        context_parts = [f"ìš”ì²­: {request}"]
        
        if knowledge:
            context_parts.append("ê´€ë ¨ ì§€ì‹:")
            for item in knowledge:
                context_parts.append(f"- {item.get('content', '')}")
        
        if tools:
            context_parts.append(f"ì‚¬ìš©í•  ë„êµ¬: {', '.join(tools)}")
        
        context = "\n".join(context_parts)
        
        prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

{context}

ë‹µë³€:"""
        
        data = {
            "model": "Qwen/Qwen3-0.6B",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 300,
            "temperature": 0.7
        }
        
        response = requests.post(self.llm_url, json=data)
        return response.json()['choices'][0]['message']['content']

# ì‚¬ìš© ì˜ˆì‹œ
agent = IntelligentAgent("ì œì¡° ì „ë¬¸ê°€")
agent.add_tool("calculator")

response = agent.process_request("ë°˜ë„ì²´ ì œì¡° ê³µì •ì˜ í’ˆì§ˆ ê´€ë¦¬ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.")
print(response)
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹…

### 1. ì„œë¹„ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸

```python
def test_connections():
    """ëª¨ë“  ì„œë¹„ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    services = {
        "PRISM-Core API": "http://localhost:8000/",
        "vLLM Service": "http://localhost:8001/v1/models",
        "Weaviate": "http://localhost:8080/v1/meta"
    }
    
    for service_name, url in services.items():
        try:
            response = requests.get(url, timeout=5)
            print(f"âœ… {service_name}: ì—°ê²° ì„±ê³µ")
        except Exception as e:
            print(f"âŒ {service_name}: ì—°ê²° ì‹¤íŒ¨ - {str(e)}")

test_connections()
```

### 2. ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

```python
def test_basic_functionality():
    """ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    # Vector DB í…ŒìŠ¤íŠ¸
    print("=== Vector DB í…ŒìŠ¤íŠ¸ ===")
    create_index("TestDocs", "í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ")
    
    test_docs = [{
        "title": "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ",
        "content": "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œì…ë‹ˆë‹¤.",
        "metadata": {"test": True}
    }]
    
    add_documents("TestDocs", test_docs)
    results = search_documents("í…ŒìŠ¤íŠ¸", "TestDocs")
    print(f"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë¬¸ì„œ")
    
    # LLM í…ŒìŠ¤íŠ¸
    print("\n=== LLM í…ŒìŠ¤íŠ¸ ===")
    response = generate_text("ì•ˆë…•í•˜ì„¸ìš”!")
    print(f"LLM ì‘ë‹µ: {response['choices'][0]['message']['content']}")

test_basic_functionality()
```

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- **API ë¬¸ì„œ**: http://localhost:8000/docs
- **Weaviate ì½˜ì†”**: http://localhost:8080
- **GitHub ì €ì¥ì†Œ**: https://github.com/PRISM-System/prism-core

---

**íŒ**: ë°ëª¨ë¥¼ ìœ„í•´ ê°„ë‹¨í•œ ì˜ˆì œë¶€í„° ì‹œì‘í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ ë³µì¡í•œ ê¸°ëŠ¥ì„ ì¶”ê°€í•´ë³´ì„¸ìš”! 